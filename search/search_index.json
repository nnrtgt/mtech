{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to M Tech Resources","title":"Welcome to M Tech Resources"},{"location":"#welcome-to-m-tech-resources","text":"","title":"Welcome to M Tech Resources"},{"location":"macshortcuts/","text":"Mac shortcut keys Goto specific path -> Command + Shift + g while in any Finder window","title":"Macshortcuts"},{"location":"semester1/","text":"","title":"Semester1"},{"location":"semester2/","text":"","title":"Semester2"},{"location":"sem1/coss/","text":"Notes and important concepts learnt during the first semester course Computer Organization and Operating Systems, M Tech Data Science from BITS Pilani. Text Books referenced: Process Scheduling Solver Process Scheduling Process Sheeduling Mobile App Page replacement algorithm solver v0.9 Gantt Chart MIPS Calculator MIPS Instruction Converter Banker's Algorithm Infix to Postfix converter Cache Calculator Cache Calculator COSS Formula Compiler Optimization transform given program to an equivalent that uses fewer resources * minimizing the program execution time * minimize the memory use * minimize the power consumed by a program 4 numeric levels - 2 bits 00 - no optimization, fastest compilation time 01 - basic optimization 02 - some optimizatin, dead code eliminatin, loop nest optimization, global register allocation, global optimization within a function scope 03 - may slow down the performances in some cases, optimizes loop nests, inverts indices, function inlining Machine independent * Constant propagation (substitute values of known constants at compile time) * Constant folding (process of recognizing and valuating constant expression at compile time) * Common Sub expression elimination (try to compute a given expression once, assuming the variables have not been modified) * Dead Code Elimination (remove the code that does not affect the program results) * Loop invariant code motion (recognizes computations within a loop that produce the same result each time the loop is executed, move the code outside the loop) * function in lining (eliminates call/return overhead, can decrease if small procedure body and few calls) Machine dependent * Instruction Scheduling * Loop unrolling(transformation technique) * Parallel unrolling Preserve Correctness Improve performance Worth the effort Execution time = number of instructions * CPI asymptomatic Big O savings are better Role of the Programmer * Readable code & Maintainable (Proecedures, Recursion etc) * Select the best algorithm * Focus on the inner loops * Eliminate optimization blockers","title":"Computer Organization and Operating Systems"},{"location":"sem1/dm/","text":"Notes and important concepts learnt during the first semester course Data Mining M Tech Data Science from BITS Pilani. for future reference. Text Book - Introduction to Data Mining, Tan, Steinbach, Karpantne, Kumar, Pearson - Second Edition Data mining is the process of automatically discovering useful information in large data repositories. Find novel and useful patterns that might otherwise remain unknown. Data Mining also predicts the outcome of a future observation. KDD - Knowledge Discovery in Databases Data mining is an integral part of knowledge discovery in databases, overall process of converting raw data into useful information. Input Data \u2192 Data Preprocessing \u2192 Data Mining \u2192 PostProcessing \u2192 Information Preprocessing include: Feature selection Dimensionality Reduction Normalization Data Subsetting Postprocessing include: Filtering Patterns Visualization Pattern Interpretation Employee Age and ID \u2013 These two attributes associated with an employee, both represented by as integers. We can talk about the average age of an employee but no average of employee ID. age have a maximum, while integers do not. Measurement scale of Centimeters vs Inches, we use the same number, but length is different when measured in Centimeters vs Inches. Properties/Operations of numbers are typically used to describe attributes: Distinctness = and <> (not equal to) Order <, <=, >, >= Addition + and \u2013 Multiplication * and / Four types of Attributes: Nominal Ordinal Interval Ratio Attribute Type Description Examples Operations Categorical (Qualitative) Nominal The values of a nominal attribute are just different names, these provide only enough information to distinguish one object from another (=, <> - Not equal to) Zip codes, employee ID, eye color, gender Mode, entropy, correlation, chi-square test Categorical (Qualitative) Ordinal The values of an ordinal attribute provide enough information to order objects (<, >) Grades (A+, A, A-, B+, B, B-), T-Shirt Size (XL, L, M, S) Median, Percentiles, Rank correlation, run tests, Sign tests Numeric (Quantitative) Interval For Interval attributes, the difference between values is meaningful, i.e., a unit of measurement exists. (+, -) Calendar dates, time, temperature in Celsius or Fahrenheit Mean, Standard deviation, Pearson\u2019s correlation, t and F tests Numeric (Quantitative) Ratio For ratio variables, both differences and ratios are meaningful (*, /) Temperature in Kelvin, monetary quantities, counts, age, mass, length, electrical current Geometric mean, harmonic mean, precent, variation Permissible Transformations: The types of attributes can also be described in terms of transformations that do not change the meaning of an attribute. For example, the meaning of a length attribute is unchanged if it is measured in meters instead of feet Describing Attributes by the Number of Values: Discrete - finite, Binary attributes (Yes,No) \u2013 Boolean Continuous \u2013 A continuous attribute is one whose values are real numbers Asymmetric Attributes: Presence of a non-zero attribute value is regarded as important. Cyclical \u2013 Time, GPS Dimensionality, Distribution and Resolution: The Dimensionality of a data set is the number of attributes that the object in the data set possess. Distribution \u2013 The distribution of a data set is the frequency of occurrence of various values or sets of values for the attributes comprising data objects. Distribution of a data set can be considered as a description of the concentration of objects in various regions of the data space. Normal (Guassian), skewness. In the distribution can make the classification difficult. Sparsity (of asymmetric attribute)\u2013 special case of skewed data. It is an advantage because usually only the non-zero values need to be stored and manipulated. Resolution \u2013 The patterns in the data also depend on the level of resolution. If the resolution is too fine, a pattern may not be visible or may be buried in noise; if the resolution is too coarse, the pattern may disappear. Types of Data Sets: Record Data Transaction or Market Basket Data The Data Matrix Ordered Data Graph Based Data Sequential Transaction Data Spatial and Spatio-Temporal Data (weather data, spatial autocorrelation) Sequence Data \u2013 DNA Time Series Data Non Record Data Spatial autocorrelation : Objects that are physically close tend to be similar in other ways as well. Analogous to Temporal autocorrelation. Data Quality Data Cleaning: Detection and correction of data quality problems Inconsistencies Human errors Limitation of measuring devices Flaws in data collection process Missing Values Measurement and Data Collection Errors: Noise - Noise is the random component of a measurement error. Precision, Bias and Accuracy: Precision \u2013 The closeness of repeated measurements to one another, often measured by standard deviation of a set of values Bias \u2013 A systematic variation of measurements from the quantity being measured, by taking the difference between the mean of the set of values and the known value of the quantity being measured Accuracy \u2013 The closeness of measurements to the true value of the quantity Outliers: data objects have characteristics that are different from most of the other data objects in the dataset. Unusual with respect to typical values for the attribute. Also known as anomalous objects or values (anamoly detection) Support Confidence Lift Missing Values Dimensionality Reduction - PCA, SVD Feature Selection Measures of Similarity and Dissimilarity: Similarity and dissimilarity are important because they are used by a number of data mining techniques, such as clustering, nearest neighbor classification and anamoly detection In many cases, the initial dataset is not needed once the similarities or dissimilarities have been computed. Proximity is used to refer to either similarity and dissimilarity. Jaccard and Cosine similarity measures are used for sparse data, such as documents, as well as correlation. Euclidean distance is used for non-sparse dense data. Mutual information can be applied to many types of data and is good for detecting Non linear relationships. Similarity between two objects is a numerical measure of the degree to which the two objects are alike. Dissimilarity between two objects is a numerical measure of the degree to which the two objects are different. Frequently, the term distance is used as a synonym for dissimilarity (special class of dissimilarities) Transformations are often applied to convert a similarity to a dissimilarity or vice versa. Similarity and Dissimilarity between Simple Attributes Dissimilarities between Data Objects Distances Euclidean distance d, between two points x and y, in one-, two-, or higher dimensional space, is given by the following formula \\(d(x,y) = \\sqrt{\\displaystyle \\sum_{k=1}^n(x_k - y_k)^2}\\) Generalized form of Euclidean distance - Minkowski distance \\(d(x,y) = { \\left( {\\displaystyle \\sum_{k=1}^n|x_k - y_k|^2}\\right)}^{1/r}\\) where r is a parameter. The following are the three most common examples of Minkowski Distances. r = 1, Manhattan distance , taxicab, City Block, \\(L_1\\) norm distance. Hamming distance , which is the number of bits that is different between two objects that have only binary attributes, i.e., between two binary vectors r = 2, Euclidean distance or \\(L_2\\) norm r = \\(\\infty\\) Supremum distance or \\(L_{max}\\) or \\(L_\\infty\\) norm Python Code to do this using scipy distancematrix r should not be confused with the number of dimensions n Properties of Euclidean distance measure: Positivity: a) d(x,y) >= 0 for all x and y b) d(x,y) = 0 only if x = y Symmetry: d(x,y) = d(y,x) for all x and y Triangle Inequality d(x,z) <= d(x,y) + d(y,z) for all points x, y and z Measures that satisfy all three properties are known as \"metrics\" The Triangle Inequality property be used while \"clustering\" technique For similarities the triangle inequality typically does not hold, but symmetry and positivity typically do. Proximity Measure: Similarity Measures for Binary Data Similarity measures between objects that contain only binary attributes are called similarity coefficients , and typically have values between 0 and 1. A value of 1 indicates that the two objects are completely similar, while a value of 0 indicates that the objects are not at all similar. Let x and y be two objects that consist of n binary attributes. The comparision of two such objects Confusion Matrix Association Rules - Market Basket transactions An association rule is an implication expression of the form \\(X \\rightarrow Y\\) , where X and Y are disjoint itemsets. i.e., \\(X \\cap Y = \\emptyset\\) The strength of an association rule can be measured in terms its support and confidence. Support determines how often a rule is applicable to a given data set, while confidence determines how frequently items in Y appear in transactions that contain X. Support \\(s(X \\rightarrow Y\\) ) Contingency Table Clustering HAC - Single linkage - Single nearest Complete linkage - farthest Max Average linkage Dendrogram - Tree like structure Difference FOIL - First Order Inductive Learner FOIL Gain Local Outlier Factor https://towardsdatascience.com/local-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843 K Distance, K nearest neighbors K-distance is the distance between the point, and it\u2019s K\u1d57\u02b0 nearest neighbor. K-neighbors denoted by N\u2096(A) includes a set of points that lie in or on the circle of radius K-distance. K-neighbors can be more than or equal to the value of K. Rechability Density \\(RD(X_i,X_j) = max(K - distance(X_j),distance(X_i,X_j) )\\)","title":"Data Mining"},{"location":"sem1/dm/#similarity-and-dissimilarity-between-simple-attributes","text":"","title":"Similarity and Dissimilarity between Simple Attributes"},{"location":"sem1/dm/#dissimilarities-between-data-objects","text":"","title":"Dissimilarities between Data Objects"},{"location":"sem1/dm/#distances","text":"Euclidean distance d, between two points x and y, in one-, two-, or higher dimensional space, is given by the following formula \\(d(x,y) = \\sqrt{\\displaystyle \\sum_{k=1}^n(x_k - y_k)^2}\\) Generalized form of Euclidean distance - Minkowski distance \\(d(x,y) = { \\left( {\\displaystyle \\sum_{k=1}^n|x_k - y_k|^2}\\right)}^{1/r}\\) where r is a parameter. The following are the three most common examples of Minkowski Distances. r = 1, Manhattan distance , taxicab, City Block, \\(L_1\\) norm distance. Hamming distance , which is the number of bits that is different between two objects that have only binary attributes, i.e., between two binary vectors r = 2, Euclidean distance or \\(L_2\\) norm r = \\(\\infty\\) Supremum distance or \\(L_{max}\\) or \\(L_\\infty\\) norm Python Code to do this using scipy distancematrix r should not be confused with the number of dimensions n Properties of Euclidean distance measure: Positivity: a) d(x,y) >= 0 for all x and y b) d(x,y) = 0 only if x = y Symmetry: d(x,y) = d(y,x) for all x and y Triangle Inequality d(x,z) <= d(x,y) + d(y,z) for all points x, y and z Measures that satisfy all three properties are known as \"metrics\" The Triangle Inequality property be used while \"clustering\" technique For similarities the triangle inequality typically does not hold, but symmetry and positivity typically do. Proximity Measure: Similarity Measures for Binary Data Similarity measures between objects that contain only binary attributes are called similarity coefficients , and typically have values between 0 and 1. A value of 1 indicates that the two objects are completely similar, while a value of 0 indicates that the objects are not at all similar. Let x and y be two objects that consist of n binary attributes. The comparision of two such objects","title":"Distances"},{"location":"sem1/dm/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"sem1/dm/#association-rules-market-basket-transactions","text":"An association rule is an implication expression of the form \\(X \\rightarrow Y\\) , where X and Y are disjoint itemsets. i.e., \\(X \\cap Y = \\emptyset\\) The strength of an association rule can be measured in terms its support and confidence. Support determines how often a rule is applicable to a given data set, while confidence determines how frequently items in Y appear in transactions that contain X. Support \\(s(X \\rightarrow Y\\) ) Contingency Table Clustering HAC - Single linkage - Single nearest Complete linkage - farthest Max Average linkage Dendrogram - Tree like structure Difference FOIL - First Order Inductive Learner FOIL Gain Local Outlier Factor https://towardsdatascience.com/local-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843 K Distance, K nearest neighbors K-distance is the distance between the point, and it\u2019s K\u1d57\u02b0 nearest neighbor. K-neighbors denoted by N\u2096(A) includes a set of points that lie in or on the circle of radius K-distance. K-neighbors can be more than or equal to the value of K. Rechability Density \\(RD(X_i,X_j) = max(K - distance(X_j),distance(X_i,X_j) )\\)","title":"Association Rules - Market Basket transactions"},{"location":"sem1/dsad/","text":"Notes and important concepts learnt during the first semester course Data Structures and Algorithm Design, M Tech Data Science from BITS Pilani. Text Books referenced: Evaluating Algorithms Data Structures, Representation, Operations and their Efficiency Linear and Non Linear Data Structures Abstract Data Type (ADT) Lists Stacks Queues Trees Binary Trees Binary Search Trees AVL Trees K-d Trees Graphs Depth First, Breadth First Directed Graph Dictionaries Hash Tables - Hashing and Collission Heaps Bloom Filters Recursion Recursion Tree Substitution Master Theorem Sorting Techniques: Quick Sort Bubble Sort Merge Sort Searching Techniques: Algorithm Design Approaches: Brute Force Method (Brute Force Algorithms are exactly what they sound like straightforward methods of solving a problem that rely on sheer computing power and trying every possibility rather than advanced techniques to improve efficiency) Greedy Method * Knapsack, MST, Dijkstra's Divide and Conquer Merge Sort, Quick Sort, Integer Multiplication Problem Dynamic Programming: (Divide and Conquer, Reuse) fractional Knapsack Knapsack 01 Matrix chain multiplication Complexity Classes NP Completeness Non Deterministic Algorithm - O(1) Non Deterministic Polynomial (NP) In computer programming, a nondeterministic algorithm is an algorithm that, even for the same input, can exhibit different behaviors on different runs, as opposed to a deterministic algorithm. hypothetical situation results can be verified in polynomial time Sudoku - Verification, once it is solved NP - Complete NP - hard Halting Problem - Given an input will my program halt, what are those input - NP Hard problem Satisfiability Cook's theorem Conjunctive normal form - Satisfiability https://www.claymath.org/millennium-problems Reduction Technique Quadratic equation to Linear Equation Show that problems are related Conjunctive Normal Form - CNF CNF - Satisfiability - CNF SAT - 3 SAT Decision and Optimization problems Consider the recurrence T(n) = T(n/2) + T(n/4) + n, T(m) =1 for m <= 5, use the substitution to the recurrence using O-notation. Polynomial Time Taking Exponential Time Taking Tractable Problem - Controllable, Manageable, Polynomial InTractable Problem - Unmanagement, Ungovernable, Exponential Complexity P Class - O(n^k) deterministic algorithm NP Class - Non-determinstic Polynomial time verified in polynomial time - Example - Sudoku 1000 x 1000","title":"Data Structures and Algorithm Design"},{"location":"sem1/mfds/","text":"Notes and important concepts learnt during the first semester course Mathematics foundations for Data science, M Tech Data Science from BITS Pilani. Text Books referenced: Math Resources Session-wise Important Concepts learnt Session 0 Matrices - rectangular array of numbers or functions Vectors - Only one row or column Operations on Matrices Multiplication A (mxn) B(nxp) if we multiply we get C(mxp) Matrix Multiplication is not commutative AB <> BA Matrix Transpose Special Matrices Symmetric \\(a_{ij} = a_{ji}\\) Skew Symmetric: \\(a_{ij} = -a_{ji}\\) Triangular Matrices Upper Traingular Matrix -> \\(a_{ij}\\) = 0 for all i > j Lower Traingular Matrix -> \\(a_{ij}\\) = 0 for all i < j Diagonal Matrix: \\(a_{ij}\\) = 0 for all i <> j Sparse Matrix: Many zeroes and few non-zero entities (Assymetric) Orthogonal matrix: Transpose (A) = Inverse(A) Elementary Row operations Interchange of rows Addition of a constant multiple of one row to another row Multiplication of a row by a nonzero constant c These operations are for rows only, not for columns Row Echelon Form (REF) Any rows of all zeroes are below any other non zero rows Each leading entry of a row is in a column to the right of the leading entry of the row above it. All Entries in a column below a leading entry are zeros Example \\(\\begin{bmatrix}3 & 2 & 0 & 7 & 9\\\\ 0 & 4 & 5 & 10 & 0 \\\\ 0 & 0 &0 & -4 & 1 \\\\0 & 0 & 0 & 0 &6 \\\\0 & 0& 0& 0&0 \\end{bmatrix}\\) Row Reduced Echelon Form (RREF) Row Echelon Form and additionally, The leading entry in each row is 1, Each leading 1 is the onlly non zero entry in its column Example \\(\\begin{bmatrix}1 & 0 & 3 & 0 & 9\\\\ 0 & 1 & 4 & 0 & -6 \\\\ 0 & 0 &0 & 1 & 1 \\\\0 & 0 & 0 & 0 &0 \\\\0 & 0& 0& 0&0 \\end{bmatrix}\\) We can transform any matrix into a matrix in reduced echelon form by using elementary row operations Rank of a Matrix The rank of a matrix A is the number of non-zero row in the RREF of A We call a matrix \\(A_1\\) row-equalent to a matrix \\(A_2\\) if \\(A_1\\) can be obtained from \\(A_2\\) by (finitely many!) elementary row operations Row-equivalent matrices have the same rank Determinant of Rank Minor Determinant of 2 x 2 submatrix is the minor of \\(a_{11}\\) \\(m_{11} = \\begin{vmatrix} a_{22} & a_{23} \\\\ a_{32} & a_{33}\\end{vmatrix}\\) Cofactor, The cofactor of an element \\(C_{ij} = (-1)^{i+j} m_{ij}\\) Session 1 Linear equations - Representation using Matrices Gauss Elimination () Operations Count In elimination procedure to get the rank we will make all elements below main diagonal zero. Multiplications and Additions required to determine the rank by elimination procedure are \\(2\\sum_{k=1}^{n-1}(n - k)(n-k+1) = O(n^3)\\) Total number of divisions are \\(O(n^2)\\) * In back substitution total number of additions, multiplications and divisions are required are \\(\\left( 2 \\sum_{k=1}^n {(n-k)} \\right) +n = O(n^2)\\) ? difference betweeen Singular and Non Singular Matrix condition number of a non-singular matrix is defined by \\(K(A) = \\|A\\| \\|A^{-1}\\|\\) conditional number of singular matrix is infinity cond(A) = \\(\\infty\\) , if A is singular ill-conditioned and well-conditioned pivoting - partial pivoting LU Factorization We write square matrix A as A = LU Dolittle's Method : L is lower triangular matrix diag(L) = 1, \\(l_{ii}=1\\) and U is the upper traingular matrix Crout's Method: U is the upper traingular matrix with diag(U) = 1, $u_{ii}=1 and L is lower triangular matrix Cholesky's method: U = L^T Decompose A such that \\(A = U^TU\\) Number of Operations and Time to Compute of various methonds compared and Cholesky's method is proven to be most efficient Iterative Methods Diagonally Dominant Matrix rank nullity theorem QR decomposition single value decomposition second derivative test ordinal and partial derivative functions eigenvalues and eigenvectors equation of the form A.x = \u03bbx A is square matrix and \u03bb is a number (scalar). x = 0 is a solution for any value of \u03bb - trivial solution For non-trivial solution i.e. x \u2260 0, the values of \u03bb are called the eigenvalues, characterstic values or latent roots of the matrix A and the corresponding solutions of the given equations A.x = \u03bbx are called eigenvectors or characteristic vectors of A. Expressed as a set of separate equations, we have","title":"Math Foundations for Data Science"},{"location":"sem1/mfds/#session-0","text":"Matrices - rectangular array of numbers or functions Vectors - Only one row or column Operations on Matrices Multiplication A (mxn) B(nxp) if we multiply we get C(mxp) Matrix Multiplication is not commutative AB <> BA Matrix Transpose Special Matrices Symmetric \\(a_{ij} = a_{ji}\\) Skew Symmetric: \\(a_{ij} = -a_{ji}\\) Triangular Matrices Upper Traingular Matrix -> \\(a_{ij}\\) = 0 for all i > j Lower Traingular Matrix -> \\(a_{ij}\\) = 0 for all i < j Diagonal Matrix: \\(a_{ij}\\) = 0 for all i <> j Sparse Matrix: Many zeroes and few non-zero entities (Assymetric) Orthogonal matrix: Transpose (A) = Inverse(A) Elementary Row operations Interchange of rows Addition of a constant multiple of one row to another row Multiplication of a row by a nonzero constant c These operations are for rows only, not for columns Row Echelon Form (REF) Any rows of all zeroes are below any other non zero rows Each leading entry of a row is in a column to the right of the leading entry of the row above it. All Entries in a column below a leading entry are zeros Example \\(\\begin{bmatrix}3 & 2 & 0 & 7 & 9\\\\ 0 & 4 & 5 & 10 & 0 \\\\ 0 & 0 &0 & -4 & 1 \\\\0 & 0 & 0 & 0 &6 \\\\0 & 0& 0& 0&0 \\end{bmatrix}\\) Row Reduced Echelon Form (RREF) Row Echelon Form and additionally, The leading entry in each row is 1, Each leading 1 is the onlly non zero entry in its column Example \\(\\begin{bmatrix}1 & 0 & 3 & 0 & 9\\\\ 0 & 1 & 4 & 0 & -6 \\\\ 0 & 0 &0 & 1 & 1 \\\\0 & 0 & 0 & 0 &0 \\\\0 & 0& 0& 0&0 \\end{bmatrix}\\) We can transform any matrix into a matrix in reduced echelon form by using elementary row operations Rank of a Matrix The rank of a matrix A is the number of non-zero row in the RREF of A We call a matrix \\(A_1\\) row-equalent to a matrix \\(A_2\\) if \\(A_1\\) can be obtained from \\(A_2\\) by (finitely many!) elementary row operations Row-equivalent matrices have the same rank Determinant of Rank Minor Determinant of 2 x 2 submatrix is the minor of \\(a_{11}\\) \\(m_{11} = \\begin{vmatrix} a_{22} & a_{23} \\\\ a_{32} & a_{33}\\end{vmatrix}\\) Cofactor, The cofactor of an element \\(C_{ij} = (-1)^{i+j} m_{ij}\\)","title":"Session 0"},{"location":"sem1/mfds/#session-1","text":"Linear equations - Representation using Matrices Gauss Elimination () Operations Count In elimination procedure to get the rank we will make all elements below main diagonal zero. Multiplications and Additions required to determine the rank by elimination procedure are \\(2\\sum_{k=1}^{n-1}(n - k)(n-k+1) = O(n^3)\\) Total number of divisions are \\(O(n^2)\\) * In back substitution total number of additions, multiplications and divisions are required are \\(\\left( 2 \\sum_{k=1}^n {(n-k)} \\right) +n = O(n^2)\\) ? difference betweeen Singular and Non Singular Matrix condition number of a non-singular matrix is defined by \\(K(A) = \\|A\\| \\|A^{-1}\\|\\) conditional number of singular matrix is infinity cond(A) = \\(\\infty\\) , if A is singular ill-conditioned and well-conditioned pivoting - partial pivoting LU Factorization We write square matrix A as A = LU Dolittle's Method : L is lower triangular matrix diag(L) = 1, \\(l_{ii}=1\\) and U is the upper traingular matrix Crout's Method: U is the upper traingular matrix with diag(U) = 1, $u_{ii}=1 and L is lower triangular matrix Cholesky's method: U = L^T Decompose A such that \\(A = U^TU\\) Number of Operations and Time to Compute of various methonds compared and Cholesky's method is proven to be most efficient Iterative Methods Diagonally Dominant Matrix rank nullity theorem QR decomposition single value decomposition second derivative test ordinal and partial derivative functions eigenvalues and eigenvectors equation of the form A.x = \u03bbx A is square matrix and \u03bb is a number (scalar). x = 0 is a solution for any value of \u03bb - trivial solution For non-trivial solution i.e. x \u2260 0, the values of \u03bb are called the eigenvalues, characterstic values or latent roots of the matrix A and the corresponding solutions of the given equations A.x = \u03bbx are called eigenvectors or characteristic vectors of A. Expressed as a set of separate equations, we have","title":"Session 1"},{"location":"sem1/mfds/resources/","text":"Discrete Mathematics - Induction Linear Independence and other topics Discreate Mathematics - Structural Induction MathSolver Graph Calculator When \\(a \\ne 0\\) , there are two solutions to \\((ax^2 + bx + c = 0)\\) and they are \\[ x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a} \\]","title":"Resources"}]}